{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LiveScatterCNN.__init__() got multiple values for argument 'interval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 118\u001b[0m\n\u001b[0;32m    116\u001b[0m LossMonitor \u001b[38;5;241m=\u001b[39m PrintLossCallback(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncode_mae\u001b[39m\u001b[38;5;124m\"\u001b[39m,interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n\u001b[0;32m    117\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStoppingByTrainMAE(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncode_mae\u001b[39m\u001b[38;5;124m\"\u001b[39m,threshold\u001b[38;5;241m=\u001b[39mlimit)\n\u001b[1;32m--> 118\u001b[0m plot\u001b[38;5;241m=\u001b[39m\u001b[43mLiveScatterCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_test_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_elements\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_test_elements\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel output shapes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[o\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mo\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel\u001b[38;5;241m.\u001b[39moutputs]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_train shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n",
      "\u001b[1;31mTypeError\u001b[0m: LiveScatterCNN.__init__() got multiple values for argument 'interval'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dense, Dropout, Concatenate, Flatten, Reshape,Conv2DTranspose\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from CallBacks import PrintLossCallback, EarlyStoppingByTrainMAE, LiveScatterTwoOutputs\n",
    "\n",
    "\n",
    "Iteration=5000\n",
    "lr=0.001\n",
    "train_size = 0.95\n",
    "limit=0.04\n",
    "l2value=0.001\n",
    "reduceRate=0.9\n",
    "modelPath=\"CNN-ANN-2-1.keras\"\n",
    "\n",
    "\n",
    "# Load node data (25 nodes, 3 features: x, y, z)\n",
    "with open(\"nodexy.txt\", \"r\") as file:\n",
    "    NodeOriginal = json.load(file)\n",
    "NodeOriginal = np.array(NodeOriginal)  # Convert to NumPy array\n",
    "# Load element data (features for each sample)\n",
    "with open(\"thickness.txt\", \"r\") as file:\n",
    "    element_features = json.load(file)\n",
    "element_features = np.array(element_features)  # Convert to NumPy array\n",
    "# Load target values\n",
    "energy = np.loadtxt(\"energy_absorbed.txt\")\n",
    "energy = np.array(energy).reshape(-1, 1)  # Ensure correct shape\n",
    "minValue=np.argmin(energy)\n",
    "mask=np.arange((len(energy))) !=minValue\n",
    "NodeOriginal=NodeOriginal[mask,:,:]\n",
    "element_features=element_features[mask,:]\n",
    "energy=energy[mask]\n",
    "# Extract x, y coordinates\n",
    "NodeOriginal_x = NodeOriginal[:, :, 0]  # Extract all x-coordinates\n",
    "NodeOriginal_y = NodeOriginal[:, :, 1]  # Extract all y-coordinates\n",
    "# Identify and remove edge nodes (x or y = 0 or 30)\n",
    "_, EdgeNodes = np.where((NodeOriginal_x == 0) | (NodeOriginal_x == 30) | (NodeOriginal_y == 0) | (NodeOriginal_y == 30))\n",
    "EdgeNodes = np.unique(EdgeNodes)  # Get unique node indices to remove\n",
    "# Create a mask and filter out edge nodes\n",
    "mask = np.ones(len(NodeOriginal_x), dtype=bool)\n",
    "mask[EdgeNodes] = False\n",
    "filtered_nodes = NodeOriginal[:, :, :]\n",
    "grid_data_list = []\n",
    "for sample in filtered_nodes:\n",
    "    node_positions = sample[:, :2]  # Extract (x, y)\n",
    "    sorted_indices = np.lexsort((node_positions[:, 0], -node_positions[:, 1]))  # Sort by y (desc), then x (asc)\n",
    "    sorted_nodes = sample[sorted_indices]  # Reorder nodes spatially\n",
    "    grid_data = sorted_nodes.reshape(5, 5, 3)  # Convert into 5x5 grid\n",
    "    grid_data_list.append(grid_data)\n",
    "node_grid_data = np.array(grid_data_list)\n",
    "\n",
    "# Normalize data\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "scaler_z = MinMaxScaler()\n",
    "element_scaler = MinMaxScaler()\n",
    "energy_scaler = MinMaxScaler()\n",
    "node_grid_data[:, :, :, 0] = scaler_x.fit_transform(node_grid_data[:, :, :, 0].reshape(-1, 1)).reshape(node_grid_data.shape[:3])\n",
    "node_grid_data[:, :, :, 1] = scaler_y.fit_transform(node_grid_data[:, :, :, 1].reshape(-1, 1)).reshape(node_grid_data.shape[:3])\n",
    "node_grid_data[:, :, :, 2] = scaler_z.fit_transform(node_grid_data[:, :, :, 2].reshape(-1, 1)).reshape(node_grid_data.shape[:3])\n",
    "element_features = element_scaler.fit_transform(element_features)\n",
    "energy = energy_scaler.fit_transform(energy)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_nodes, X_test_nodes, X_train_elements, X_test_elements, y_train, y_test = train_test_split(\n",
    "    node_grid_data, element_features, energy, test_size=1 - train_size, random_state=42)\n",
    "\n",
    "# Define CNN model for node features\n",
    "node_input = Input(shape=(5,5, 3), name=\"Node_Input\")\n",
    "node_branch = Conv2D(filters=5, kernel_size=(3, 3), activation=\"swish\", padding=\"same\")(node_input)\n",
    "node_branch = Conv2D(filters=10, kernel_size=(3, 3), activation=\"swish\", padding=\"same\")(node_branch)\n",
    "node_branch = Conv2D(filters=15, kernel_size=(3, 3), activation=\"swish\", padding=\"same\")(node_branch)\n",
    "node_branch = GlobalAveragePooling2D()(node_branch)  # Reduce to 1D\n",
    "# Define ANN model for element features\n",
    "element_input = Input(shape=(X_train_elements.shape[1],), name=\"Element_Input\")\n",
    "element_branch = Dense(32, activation=\"swish\",kernel_regularizer=l2(l2value))(element_input)\n",
    "element_branch = Dense(24, activation=\"swish\",kernel_regularizer=l2(l2value))(element_branch)\n",
    "element_branch = Dense(16, activation=\"swish\",kernel_regularizer=l2(l2value))(element_branch)\n",
    "\n",
    "# Concatenate CNN and ANN features\n",
    "merged = Concatenate()([node_branch, element_branch])\n",
    "latentSpace=Dense(8,activation=\"swish\",name=\"Latent\",kernel_regularizer=l2(l2value))(merged)\n",
    "merged = Dense(4, activation=\"swish\",kernel_regularizer=l2(l2value))(latentSpace)\n",
    "merged = Dense(2, activation=\"swish\",kernel_regularizer=l2(l2value))(merged)\n",
    "encode = Dense(1, activation=\"linear\", name=\"Encode\",kernel_regularizer=l2(l2value))(merged)\n",
    "\n",
    "decoded = Dense(20, activation=\"swish\",kernel_regularizer=l2(l2value))(latentSpace)\n",
    "decoded = Dense(50, activation=\"swish\",kernel_regularizer=l2(l2value))(decoded)\n",
    "decoded = Dense(25*3, activation=\"swish\",kernel_regularizer=l2(l2value))(decoded)\n",
    "decoded = Reshape((5,5,3))(decoded)\n",
    "decoded = Conv2DTranspose(8, kernel_size=(3,3), activation=\"swish\", padding=\"same\")(decoded)\n",
    "decoded = Conv2DTranspose(3, kernel_size=(3,3), activation=\"swish\", padding=\"same\")(decoded)\n",
    "decode = Reshape((5,5,3), name=\"Decode\")(decoded)\n",
    "\n",
    "# Create model\n",
    "if os.path.exists(modelPath):\n",
    "    print(\"Loading model\")\n",
    "    model=load_model(modelPath)\n",
    "else:\n",
    "    print(\"Creating model\")\n",
    "    model = Model(inputs=[node_input, element_input], outputs=[encode,decode])\n",
    "model.compile(optimizer=Adam(learning_rate=lr), \n",
    "              loss={\"Encode\": \"mse\", \"Decode\": \"mse\"},\n",
    "              metrics={\"Encode\": [\"mae\", \"mse\"], \"Decode\": [\"mae\", \"mse\"]})\n",
    "\n",
    "# Define callbacks for early stopping and learning rate reduction\n",
    "checkpoint = ModelCheckpoint(modelPath, monitor=\"Encode_mae\", save_best_only=True, mode=\"min\", verbose=0)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"Encode_mae\", factor=reduceRate, patience=200, min_lr=1e-6, verbose=0)\n",
    "X_train_nodes = np.array(X_train_nodes, dtype=np.float32)\n",
    "LossMonitor = PrintLossCallback(\"Encode_mae\",interval=500)\n",
    "early_stopping = EarlyStoppingByTrainMAE(\"Encode_mae\",threshold=limit)\n",
    "plot=LiveScatterTwoOutputs(X_train_nodes,X_test_nodes, X_train_elements,X_test_elements, y_train, y_test, interval=10)\n",
    "\n",
    "print(f\"Model output shapes: {[o.shape for o in model.outputs]}\")\n",
    "print(f\"y_train shape: {y_train.shape}\") \n",
    "print(f\"X_train_nodes shape: {X_train_nodes.shape}\")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    [X_train_nodes, X_train_elements],\n",
    "    [y_train, X_train_nodes],\n",
    "    epochs=Iteration,\n",
    "    batch_size=25,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[checkpoint,LossMonitor,reduce_lr,plot],\n",
    "    verbose=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
