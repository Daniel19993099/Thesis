{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_nodes shape: (8639, 5, 5, 2)\n",
      "X_train_elements shape: (8639, 36)\n",
      "A_connectivity shape: (25, 25)\n",
      "A_thickness shape: (25, 25)\n",
      "y_train shape: (8639, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling GCNConv.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'gcn_conv_56' (of type GCNConv). Either the `GCNConv.call()` method is incorrect, or you need to implement the `GCNConv.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nTried to convert 'y' to a tensor and failed. Error: None values not supported.\u001b[0m\n\nArguments received by GCNConv.call():\n  • args=(['<KerasTensor shape=(None, 25, 2), dtype=float32, sparse=False, name=GNN_Node_Input>', '<KerasTensor shape=(None, 25, 25), dtype=float32, sparse=False, name=Adjacency_Connectivity>'],)\n  • kwargs={'mask': ['None', 'None']}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 147\u001b[0m\n\u001b[0;32m    144\u001b[0m adj_input \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m25\u001b[39m),batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdjacency_Connectivity\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Now supports batching\u001b[39;00m\n\u001b[0;32m    145\u001b[0m adj_thickness_input \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m25\u001b[39m),batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdjacency_Thickness\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 147\u001b[0m gnn_branch \u001b[38;5;241m=\u001b[39m \u001b[43mGCNConv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgnn_node_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43madj_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m gnn_branch \u001b[38;5;241m=\u001b[39m GCNConv(\u001b[38;5;241m100\u001b[39m, activation\u001b[38;5;241m=\u001b[39mact)([adj_input, adj_thickness_input])\n\u001b[0;32m    149\u001b[0m gnn_branch \u001b[38;5;241m=\u001b[39m Flatten()(gnn_branch)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\spektral\\layers\\convolutional\\conv.py:74\u001b[0m, in \u001b[0;36mcheck_dtypes_decorator.<locals>._inner_check_dtypes\u001b[1;34m(inputs, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(call)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inner_check_dtypes\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     73\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m check_dtypes(inputs)\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\spektral\\layers\\convolutional\\gcn_conv.py:106\u001b[0m, in \u001b[0;36mGCNConv.call\u001b[1;34m(self, inputs, mask)\u001b[0m\n\u001b[0;32m    104\u001b[0m     output \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mbias_add(output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 106\u001b[0m     \u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    107\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(output)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling GCNConv.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'gcn_conv_56' (of type GCNConv). Either the `GCNConv.call()` method is incorrect, or you need to implement the `GCNConv.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nTried to convert 'y' to a tensor and failed. Error: None values not supported.\u001b[0m\n\nArguments received by GCNConv.call():\n  • args=(['<KerasTensor shape=(None, 25, 2), dtype=float32, sparse=False, name=GNN_Node_Input>', '<KerasTensor shape=(None, 25, 25), dtype=float32, sparse=False, name=Adjacency_Connectivity>'],)\n  • kwargs={'mask': ['None', 'None']}"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dense, Dropout, Concatenate, Flatten, Reshape, Conv2DTranspose, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spektral.layers import GCNConv\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "\n",
    "# Hyperparameters\n",
    "Iteration = 2000\n",
    "lr = 0.0001\n",
    "train_size = 0.96\n",
    "l2value = 0.005\n",
    "reduceRate = 0.9\n",
    "modelPath = \"CNN-GNN.keras\"\n",
    "act = \"swish\"\n",
    "batch = 50\n",
    "initializer = GlorotUniform()\n",
    "drop = 0.3\n",
    "\n",
    "# Load node data (25 nodes, 3 features: x, y, z)\n",
    "with open(\"nodexy.txt\", \"r\") as file:\n",
    "    NodeOriginal = json.load(file)\n",
    "NodeOriginal = np.array(NodeOriginal)  # Convert to NumPy array\n",
    "# Load element data (features for each sample)\n",
    "with open(\"thickness.txt\", \"r\") as file:\n",
    "    element_features = json.load(file)\n",
    "element_features = np.array(element_features)  # Convert to NumPy array\n",
    "# Load target values\n",
    "energy = np.loadtxt(\"energy_absorbed.txt\")\n",
    "energy = np.array(energy).reshape(-1, 1)  # Ensure correct shape\n",
    "minValue=np.argmin(energy)\n",
    "mask=np.arange((len(energy))) !=minValue\n",
    "NodeOriginal=NodeOriginal[mask,:,:]\n",
    "element_features=element_features[mask,:]\n",
    "energy=energy[mask]\n",
    "# Extract x, y coordinates\n",
    "NodeOriginal_x = NodeOriginal[:, :, 0]  # Extract all x-coordinates\n",
    "NodeOriginal_y = NodeOriginal[:, :, 1]  # Extract all y-coordinates\n",
    "# Identify and remove edge nodes (x or y = 0 or 30)\n",
    "_, EdgeNodes = np.where((NodeOriginal_x == 0) | (NodeOriginal_x == 30) | (NodeOriginal_y == 0) | (NodeOriginal_y == 30))\n",
    "EdgeNodes = np.unique(EdgeNodes)  # Get unique node indices to remove\n",
    "# Create a mask and filter out edge nodes\n",
    "mask = np.ones(len(NodeOriginal_x), dtype=bool)\n",
    "mask[EdgeNodes] = False\n",
    "filtered_nodes = NodeOriginal[:, :, :]\n",
    "grid_data_list = []\n",
    "for sample in filtered_nodes:\n",
    "    node_positions = sample[:, :2]  # Extract (x, y)\n",
    "    sorted_indices = np.lexsort((node_positions[:, 0], -node_positions[:, 1]))  # Sort by y (desc), then x (asc)\n",
    "    sorted_nodes = sample[sorted_indices]  # Reorder nodes spatially\n",
    "    grid_data = sorted_nodes.reshape(5, 5, 3)  # Convert into 5x5 grid\n",
    "    grid_data_list.append(grid_data)\n",
    "node_grid_data = np.array(grid_data_list)\n",
    "\n",
    "# Normalize data\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "scaler_z = MinMaxScaler()\n",
    "element_scaler = MinMaxScaler()\n",
    "energy_scaler = MinMaxScaler()\n",
    "node_grid_data[:, :, :, 0] = scaler_x.fit_transform(node_grid_data[:, :, :, 0].reshape(-1, 1)).reshape(node_grid_data.shape[:3])\n",
    "node_grid_data[:, :, :, 1] = scaler_y.fit_transform(node_grid_data[:, :, :, 1].reshape(-1, 1)).reshape(node_grid_data.shape[:3])\n",
    "node_grid_data[:, :, :, 2] = scaler_z.fit_transform(node_grid_data[:, :, :, 2].reshape(-1, 1)).reshape(node_grid_data.shape[:3])\n",
    "node_grid_data=node_grid_data[:, : , : , 0:2]\n",
    "element_features = element_scaler.fit_transform(element_features)\n",
    "energy = energy_scaler.fit_transform(energy)\n",
    "\n",
    "# Split data\n",
    "X_train_nodes, X_test_nodes, X_train_elements, X_test_elements, y_train, y_test = train_test_split(\n",
    "    node_grid_data, element_features, energy, test_size=1 - train_size, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "def parse_inp_file(filename):\n",
    "    connectivity_list = []\n",
    "    inside_element_section = False  # Flag to track when inside *Element section\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "         for line in file:\n",
    "            line = line.strip()  # Remove leading/trailing spaces\n",
    "                \n",
    "            # Detect start of element section\n",
    "            if line.startswith(\"*Element\"):\n",
    "                inside_element_section = True\n",
    "                continue  # Move to next line\n",
    "                # Detect end of element section (if another keyword appears)\n",
    "            if inside_element_section and line.startswith(\"*\"):\n",
    "                break  # Stop reading elements\n",
    "            if inside_element_section and line:\n",
    "                parts = line.split(\",\")  # Split by commas\n",
    "                nodes = list(map(int, parts[1:]))  # Convert remaining parts to integers\n",
    "                node1, node2 = nodes[0], nodes[1]\n",
    "                connectivity_list.append((node1 - 1, node2 - 1))\n",
    "    return connectivity_list\n",
    "\n",
    "\n",
    "inp_file_path = \"ExplcitBeam.inp\"\n",
    "connectivity_list = parse_inp_file(inp_file_path)\n",
    "\n",
    "def create_adjacency_matrices(connectivity_list, num_nodes, element_thickness):\n",
    "    A_connectivity = np.zeros((num_nodes, num_nodes))\n",
    "    A_thickness = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "    for (i, j) in connectivity_list:\n",
    "        A_connectivity[i, j] = A_connectivity[j, i] = 1\n",
    "        A_thickness[i, j] = A_thickness[j, i] = element_thickness[i, 0]  # Assign thickness value\n",
    "\n",
    "    A_thickness = A_thickness / np.max(A_thickness)  # Normalize\n",
    "    return A_connectivity, A_thickness\n",
    "\n",
    "\n",
    "A_connectivity, A_thickness = create_adjacency_matrices(connectivity_list, 25, element_features)\n",
    "A_connectivity = tf.convert_to_tensor(A_connectivity, dtype=tf.float32)\n",
    "A_thickness = tf.convert_to_tensor(A_thickness, dtype=tf.float32)\n",
    "\n",
    "print(f\"X_train_nodes shape: {X_train_nodes.shape}\")  # Should be (batch_size, 5, 5, 2)\n",
    "print(f\"X_train_elements shape: {X_train_elements.shape}\")  # Should be (batch_size, num_features)\n",
    "print(f\"A_connectivity shape: {A_connectivity.shape}\")  # Should be (batch_size, 25, 25)\n",
    "print(f\"A_thickness shape: {A_thickness.shape}\")  # Should be (batch_size, 25, 25)\n",
    "print(f\"y_train shape: {y_train.shape}\")  # Should NOT be None\n",
    "\n",
    "node_input = Input(shape=(5, 5, 2), name=\"Node_Input\")\n",
    "node_branch = Conv2D(16, kernel_size= (3, 3), padding=\"same\", activation=act, kernel_initializer=initializer)(node_input)\n",
    "node_branch = BatchNormalization()(node_branch)\n",
    "node_branch = GlobalAveragePooling2D()(node_branch)\n",
    "\n",
    "\n",
    "\n",
    "element_input = Input(shape=(X_train_elements.shape[1],), name=\"Element_Input\")\n",
    "element_branch = Dense(50, activation=act, kernel_regularizer=l2(l2value), kernel_initializer=initializer)(element_input)\n",
    "element_branch = Dropout(drop)(element_branch)\n",
    "element_branch = Dense(16, activation=act, kernel_initializer=initializer)(element_branch)\n",
    "\n",
    "\n",
    "gnn_node_input = Input(shape=(25, 2),batch_size=None, name=\"GNN_Node_Input\")\n",
    "adj_input = Input(shape=(25, 25),batch_size=None, name=\"Adjacency_Connectivity\")  # Now supports batching\n",
    "adj_thickness_input = Input(shape=(25, 25),batch_size=1, name=\"Adjacency_Thickness\")\n",
    "\n",
    "gnn_branch = GCNConv(25, activation=act)([gnn_node_input,adj_input])\n",
    "gnn_branch = GCNConv(100, activation=act)([adj_input, adj_thickness_input])\n",
    "gnn_branch = Flatten()(gnn_branch)\n",
    "\n",
    "# Merge CNN, ANN, and GNN\n",
    "merged = Concatenate()([node_branch, element_branch, gnn_branch])\n",
    "\n",
    "# Latent Space\n",
    "latentSpace = Dense(50, activation=act, kernel_initializer=initializer)(merged)\n",
    "latentSpace = Dropout(drop)(latentSpace)\n",
    "latentSpace = Dense(10, activation=act, name=\"Latent\")(latentSpace)\n",
    "\n",
    "encode = Dense(1, activation=\"linear\", name=\"Encode\")(latentSpace)\n",
    "\n",
    "# Decoding Output\n",
    "decoded = Dense(30, activation=act, kernel_initializer=initializer)(latentSpace)\n",
    "decoded = Dense(25 * 2, activation=act, kernel_initializer=initializer)(decoded)\n",
    "decoded = Reshape((5, 5, 2))(decoded)\n",
    "decode = Conv2DTranspose(2, (3, 3), activation=act, padding=\"same\", kernel_initializer=initializer, name=\"Decode\")(decoded)\n",
    "\n",
    "# Create Model\n",
    "model = Model(inputs=[node_input, element_input, gnn_node_input, adj_input, adj_thickness_input], outputs=[encode, decode])\n",
    "model.compile(optimizer=Adam(learning_rate=lr), loss={\"Encode\": \"mse\", \"Decode\": \"mse\"})\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    [X_train_nodes, X_train_elements, X_train_nodes.reshape(-1, 25, 2), A_connectivity, A_thickness],\n",
    "    [y_train, X_train_nodes],\n",
    "    epochs=Iteration,\n",
    "    batch_size=batch,\n",
    "    validation_split=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m adj_input \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(num_nodes, num_nodes), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdjacency_Matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m mask_input \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(num_nodes,), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMask\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Mask input\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m mask_float \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size, num_nodes, 1)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# GCN with mask\u001b[39;00m\n\u001b[0;32m     29\u001b[0m gcn \u001b[38;5;241m=\u001b[39m GCNConv(\u001b[38;5;241m16\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m)([node_input, adj_input], mask\u001b[38;5;241m=\u001b[39mmask_float)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:88\u001b[0m, in \u001b[0;36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     87\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     90\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend\\common\\keras_tensor.py:138\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[1;34m(self, dtype, name)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from spektral.layers import GCNConv\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define number of nodes and features\n",
    "batch_size = 4\n",
    "num_nodes = 5\n",
    "num_features = 3\n",
    "\n",
    "# Node features (batch_size, num_nodes, num_features)\n",
    "X = np.random.rand(batch_size, num_nodes, num_features).astype(\"float32\")\n",
    "\n",
    "# Adjacency matrix (batch_size, num_nodes, num_nodes)\n",
    "A = np.random.randint(0, 2, (batch_size, num_nodes, num_nodes)).astype(\"float32\")\n",
    "\n",
    "# Mask (batch_size, num_nodes) - Example: masking the last node in each graph\n",
    "mask = np.ones((batch_size, num_nodes), dtype=\"float32\")\n",
    "mask[:, -1] = 0  # Mark last node in every graph as ignored\n",
    "\n",
    "# Define Input layers\n",
    "node_input = Input(shape=(num_nodes, num_features), name=\"Node_Features\")\n",
    "adj_input = Input(shape=(num_nodes, num_nodes), name=\"Adjacency_Matrix\")\n",
    "mask_input = Input(shape=(num_nodes,), dtype=tf.float32, name=\"Mask\")  # Mask input\n",
    "mask_float = tf.reshape(mask_input, (-1, num_nodes, 1))  # (batch_size, num_nodes, 1)\n",
    "\n",
    "# GCN with mask\n",
    "gcn = GCNConv(16, activation=\"relu\")([node_input, adj_input], mask=mask_float)\n",
    "gcn = GCNConv(8, activation=\"relu\")([gcn, adj_input], mask=mask_input)\n",
    "gcn = Flatten()(gcn)\n",
    "\n",
    "# Output\n",
    "output = Dense(1, activation=\"linear\")(gcn)\n",
    "\n",
    "# Create Model\n",
    "model = Model(inputs=[node_input, adj_input, mask_input], outputs=output)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "model.summary()\n",
    "\n",
    "# Train Model\n",
    "history = model.fit([X, A, mask], np.random.rand(batch_size, 1), epochs=5, verbose=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
